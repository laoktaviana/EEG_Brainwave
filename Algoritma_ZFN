import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import random
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, GlobalAveragePooling1D, LSTM, Flatten
from tensorflow.keras.models import Model
import gdown # Import gdown

def create_folder_if_not_exists(folder_path):
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        print(f"Folder '{folder_path}' created.")

def load_and_prepare_data(google_drive_url, extract_to):
    csv_file_name = 'emotions.csv'
    csv_file_path = os.path.join(extract_to, csv_file_name) # Save in the extracted_files directory

    # Extract file ID from the Google Drive URL
    file_id = google_drive_url.split('/')[-2]
    direct_download_url = f'https://drive.google.com/uc?export=download&id={file_id}'

    # Create the extract_to folder if it doesn't exist
    create_folder_if_not_exists(extract_to)

    # Check if the CSV file exists before attempting to download
    if not os.path.isfile(csv_file_path):
        print(f"File {csv_file_name} not found locally. Downloading dataset from Google Drive...")
        try:
            # Use gdown to download the file from Google Drive
            gdown.download(direct_download_url, csv_file_path, quiet=False)
            print("Download complete.")
        except Exception as e:
            print(f"Error during Google Drive download: {e}")
            raise FileNotFoundError(f"Failed to download {csv_file_name} from Google Drive. Check the URL and your internet connection.")

    # Re-check if the file exists after attempting the download
    if os.path.isfile(csv_file_path):
        print(f"Loading dataset from {csv_file_path}...")
        df = pd.read_csv(csv_file_path)
        print("Dataset successfully loaded.")
        return df
    else:
        # This part should ideally not be reached if the download was successful
        raise FileNotFoundError(f"File {csv_file_path} still not found after attempted download.")


def preprocess_data(df):
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    X = X / np.where(X.max(axis=0) == 0, 1, X.max(axis=0))

    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

    xtrain, xtest, ytrain, ytest = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"xtrain shape: {xtrain.shape}")
    print(f"xtest shape: {xtest.shape}")
    return xtrain, xtest, ytrain, ytest, label_encoder

def create_ddn_model(input_shape, num_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_shape,)),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    return model

def create_zfn(input_shape, num_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_shape, 1)),  # Input with extra dimension
        tf.keras.layers.Lambda(lambda x: tf.squeeze(x, axis=-1)),  # Removing extra dimension
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    return model

def create_pretrained_cnn(input_shape, num_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Reshape((input_shape, 1), input_shape=(input_shape,)),  # Reshape for 1D CNN
        Conv1D(32, kernel_size=3, activation='relu', padding='same'),
        MaxPooling1D(pool_size=2),
        Conv1D(64, kernel_size=3, activation='relu', padding='same'),
        MaxPooling1D(pool_size=2),
        Conv1D(128, kernel_size=3, activation='relu', padding='same'),
        GlobalAveragePooling1D(),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    return model

def create_pretrained_rnn(input_shape, num_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_shape, 1)),
        LSTM(128, return_sequences=True),
        LSTM(64),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    return model

def create_pretrained_dnn(input_shape, num_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_shape,)),
        Dense(512, activation='relu'),
        Dropout(0.3),
        Dense(256, activation='relu'),
        Dropout(0.3),
        Dense(128, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    return model


def plot_training_metrics(history, output_folder, model_name):
    plt.figure()
    plt.plot(history.history['accuracy'], label='Training Accuracy', color='#8B0000')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linestyle='--', color='#7f7f7f')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.savefig(os.path.join(output_folder, f'accuracy_{model_name}.png'), bbox_inches="tight", dpi=1000)
    plt.close()

    plt.figure()
    plt.plot(history.history['loss'], label='Training Loss', color='#8B0000')
    plt.plot(history.history['val_loss'], label='Validation Loss', linestyle='--', color='#7f7f7f')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(os.path.join(output_folder, f'loss_{model_name}.png'), bbox_inches="tight", dpi=100)
    plt.close()

def plot_confusion_matrix(cm, class_names, output_folder, model_name):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100, annot=True, fmt=".2f", cmap="PuRd", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.savefig(os.path.join(output_folder, f'confusion_matrix_{model_name}.png'), bbox_inches="tight", dpi=1000)
    plt.close()

def train_and_evaluate_model(model, xtrain, ytrain, xtest, ytest, output_folder, label_encoder, model_name):
    import time

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=10,
        restore_best_weights=True)

    print("Training started...")
    start = time.time()

    history = model.fit(
        xtrain, ytrain,
        validation_split=0.2,
        batch_size=32,
        epochs=100,
        callbacks=[early_stopping],
        verbose=1)

    end = time.time()
    training_time = end - start
    epoch_count = len(history.history['loss'])
    time_per_epoch = training_time / epoch_count

    final_loss = history.history['loss'][-1]
    final_accuracy = history.history['accuracy'][-1]

    print(f"Total training time: {training_time:.4f} seconds")
    print(f"Time per epoch: {time_per_epoch:.4f} seconds")
    print(f"Epochs: {epoch_count}")
    print(f"Final loss: {final_loss:.4f}")
    print(f"Final accuracy: {final_accuracy:.4f}")

    # Save to file
    with open(os.path.join(output_folder, f"training_time_{model_name}.txt"), "w") as f:
        f.write(f"Total training time (s): {training_time:.4f}\n")
        f.write(f"Time per epoch (s): {time_per_epoch:.4f}\n")
        f.write(f"Epochs: {epoch_count}\n")
        f.write(f"Final loss: {final_loss:.4f}\n")
        f.write(f"Final accuracy: {final_accuracy:.4f}\n")


    plot_training_metrics(history, output_folder, model_name)

    model.save(os.path.join(output_folder, f"eeg_model_{model_name}.keras"))

    with open(os.path.join(output_folder, f"model_summary_{model_name}.txt"), "w") as f:
        model.summary(print_fn=lambda x: f.write(x + '\n'))

    y_pred = model.predict(xtest)
    y_pred_classes = np.argmax(y_pred, axis=1)

    cm = confusion_matrix(ytest, y_pred_classes)
    plot_confusion_matrix(cm, label_encoder.classes_, output_folder, model_name)

    report = classification_report(ytest, y_pred_classes, target_names=label_encoder.classes_)
    with open(os.path.join(output_folder, f"classification_report_{model_name}.txt"), "w") as f:
        f.write(report)

    # Set global seed to ensure reproducible results
def set_global_seed(seed=42):
    import random
    random.seed(seed)
    np.random.seed(42)
    tf.random.set_seed(42)

set_global_seed()

if __name__ == "__main__":
    google_drive_url = 'https://drive.google.com/file/d/16zcs_3rBjBCmIyBujTN2mU99Koqp8yYl/view?usp=share_link'
    extract_to = './extracted_files'
    output_folder = './output_results'

    create_folder_if_not_exists(output_folder)
    create_folder_if_not_exists(extract_to)

    df = load_and_prepare_data(google_drive_url, extract_to)
    xtrain, xtest, ytrain, ytest, label_encoder = preprocess_data(df)

    model_type = input("Choose a model (ddn/cnn/rnn/dnn/zfn): ").strip().lower()

    if model_type == "ddn":
        model = create_ddn_model(xtrain.shape[1], len(set(ytrain)))
    elif model_type == "cnn":
        model = create_pretrained_cnn(xtrain.shape[1], len(set(ytrain)))
    elif model_type == "rnn":
        model = create_pretrained_rnn(xtrain.shape[1], len(set(ytrain)))
    elif model_type == "dnn":
        model = create_pretrained_dnn(xtrain.shape[1], len(set(ytrain)))
    elif model_type == "zfn":
         model = create_zfn(xtrain.shape[1], len(set(ytrain)))
    else:
        raise ValueError("Invalid model! Choose between 'ddn', 'cnn', 'rnn', 'dnn', or 'zfn'.")

    train_and_evaluate_model(model, xtrain, ytrain, xtest, ytest, output_folder, label_encoder, model_type)
